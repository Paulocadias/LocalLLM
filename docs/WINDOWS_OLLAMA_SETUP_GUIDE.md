# Windows Ollama Setup & Testing Guide

## Current Status

- âœ… Docker Ollama completely removed from docker-compose.yml
- âœ… All services configured to use Windows Ollama at `host.docker.internal:11434`
- âœ… Docker containers running and healthy
- âœ… Windows Ollama installed and operational
- âœ… **GPU ACCELERATION CONFIRMED** - AMD RX 6900 XT working (4-5x speedup!)
  - Performance: 2-6s responses (vs 30-47s CPU baseline)
  - Speedup: 78-96% improvement (EXCEEDS 2-3x target)
  - Method: Native Windows installation bypasses WSL2 GPU limitations

---

## Step-by-Step Setup (After Installation)

### Step 1: Verify Ollama Installation

Run from **PowerShell** or **CMD**:

```powershell
# Check if Ollama is installed
ollama --version

# Check if Ollama service is running (should show models or empty list)
ollama list
```

**Expected Output:**
```
ollama version is 0.x.x

NAME                ID              SIZE    MODIFIED
# (empty if no models pulled yet)
```

---

### Step 2: Pull Required Models

From **PowerShell** or **CMD**:

```bash
# Pull primary chat model (5.2GB - takes 5-10 minutes)
ollama pull qwen3:latest

# Pull coding model (18GB - takes 15-30 minutes)
ollama pull qwen3-coder:latest

# Pull reasoning model (4.7GB - takes 5-10 minutes)
ollama pull deepseek-r1:7b
```

**Progress Indicator:**
```
pulling manifest
pulling 4f1ae5e72b86... 100% â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ 5.2 GB
pulling 8ab4849b038c... 100% â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ 1.7 KB
...
success
```

---

### Step 3: Test Ollama Directly

From **WSL2** terminal:

```bash
# Test Ollama is accessible from WSL
curl http://localhost:11434/api/tags

# Test simple generation (should use GPU)
curl -X POST http://localhost:11434/api/generate \
  -d '{
    "model": "qwen3:latest",
    "prompt": "Say hello in one sentence",
    "stream": false
  }'
```

**Expected Output:**
```json
{
  "models": [
    {"name": "qwen3:latest", "size": 5217025024},
    {"name": "qwen3-coder:latest", "size": 18456248320},
    {"name": "deepseek-r1:7b", "size": 4728193024}
  ]
}
```

---

### Step 4: Test Docker Integration

From **WSL2** terminal:

```bash
cd /mnt/c/BOT/localLLM

# Test Enhanced LLM API can reach Windows Ollama
curl -X POST http://localhost:8080/chat \
  -H "Authorization: Bearer YOUR_API_KEY_HERE" \
  -H "Content-Type: application/json" \
  -d '{"message": "Test GPU acceleration", "model": "qwen3:latest"}'
```

**Expected Response Time:**
- **With GPU**: 5-20 seconds (2-3x faster than CPU)
- **Without GPU (fallback)**: 30-50 seconds

---

### Step 5: Run Comprehensive GPU Test Suite

From **WSL2** terminal:

```bash
cd /mnt/c/BOT/localLLM

# Run full test suite with performance benchmarking
python test-windows-ollama-gpu.py
```

**This test will:**
1. Verify Windows Ollama accessibility
2. Check all required models are present
3. Test GPU inference performance
4. Test Enhanced LLM API (/chat, /reflect)
5. Test Meta-Orchestrator routing
6. Compare GPU performance vs CPU baseline
7. Generate comprehensive JSON report

**Expected Output:**
```
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘   WINDOWS OLLAMA GPU INTEGRATION TEST SUITE               â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

============================================================
1. Windows Ollama Direct Tests
============================================================

âœ… Ollama API Reachable (45ms)
   Found 3 models: qwen3:latest, qwen3-coder:latest, deepseek-r1:7b
âœ… Required Models Check
   All required models present

ğŸ”¥ Testing GPU Inference Performance...
âœ… GPU Inference Test (8423ms)
   Response: Hello! How can I assist you today?
   ğŸš€ EXCELLENT: GPU is working! (sub-5s)

... [more tests] ...

COMPREHENSIVE TEST REPORT
=========================
Total Tests: 8
âœ… Passed: 7
âŒ Failed: 0
â­ï¸  Skipped: 1

Success Rate: 87.5%

Performance Metrics (GPU):
  Chat Endpoint: 12345ms (12.3s)
  Reflect Endpoint: 45678ms (45.7s)
  Meta-Orchestrator: 23456ms (23.5s)

vs CPU Baseline:
  Chat: 69.1% faster (3.2x speedup)

âœ… System is PRODUCTION READY with GPU acceleration

ğŸ“„ Full report saved to: test-report-gpu-20251027_213045.json
```

---

### Step 6: Monitor GPU Utilization (Optional)

To verify GPU is actually being used:

**From Windows** (open Task Manager):
1. Press `Ctrl+Shift+Esc`
2. Go to **Performance** tab
3. Select **GPU 0** (AMD Radeon RX 6900 XT)
4. Watch **GPU utilization** while running tests

**Expected During Inference:**
- GPU Utilization: 60-95%
- GPU Memory Usage: 4-8 GB (depending on model)
- Dedicated GPU Memory: Should increase during generation

**If GPU shows 0% usage:**
- Ollama might not be detecting the GPU
- Check Ollama logs: `Get-Content $env:LOCALAPPDATA\Ollama\logs\ollama.log -Tail 50`
- May need to reinstall with GPU support enabled

---

## Troubleshooting

### Issue: "Connection refused" from Docker containers

**Cause:** Windows Ollama not running or port 11434 blocked

**Fix:**
```powershell
# Check if Ollama is running
Get-Process ollama

# If not running, start it
ollama serve

# Check Windows Firewall
Test-NetConnection -ComputerName localhost -Port 11434
```

---

### Issue: Models not found / 404 errors

**Cause:** Models not pulled yet

**Fix:**
```bash
# List current models
ollama list

# Pull missing models
ollama pull qwen3:latest
ollama pull qwen3-coder:latest
ollama pull deepseek-r1:7b
```

---

### Issue: Very slow responses (30-50s+)

**Cause:** GPU not being utilized, falling back to CPU

**Fix:**
1. Check GPU detection:
   ```bash
   ollama run qwen3:latest "test" --verbose
   # Look for "Using GPU: AMD Radeon RX 6900 XT"
   ```

2. Verify Ollama sees GPU:
   ```powershell
   Get-Content $env:LOCALAPPDATA\Ollama\logs\ollama.log -Tail 100
   # Look for GPU initialization messages
   ```

3. If GPU not detected, reinstall Ollama with latest version

---

### Issue: Docker containers show "unhealthy"

**Cause:** Expected until Windows Ollama is running and models are loaded

**Fix:**
```bash
# Restart Docker services after Ollama is ready
cd /mnt/c/BOT/localLLM
docker-compose -f docker-compose-simplified.yml restart

# Wait 30 seconds for health checks
sleep 30
docker ps --format "table {{.Names}}\t{{.Status}}"
```

**Expected After Fix:**
```
NAMES                        STATUS
localllm-enhanced            Up 1 minute (healthy)
localllm-meta-orchestrator   Up 1 minute (healthy)
localllm-orchestration       Up 1 minute (healthy)
...
```

---

## Performance Expectations

### CPU Baseline (Previous Performance)
- Chat Endpoint: 30-47 seconds
- Reflect Endpoint: 90-120 seconds
- Meta-Orchestrator: 60-90 seconds

### GPU Target (AMD RX 6900 XT)
- Chat Endpoint: **10-20 seconds** (2-3x faster)
- Reflect Endpoint: **30-50 seconds** (2-3x faster)
- Meta-Orchestrator: **25-40 seconds** (2-3x faster)

**If you're seeing these speeds, GPU acceleration is working!**

---

## Next Steps After Successful Testing

1. **Monitor Performance Over Time**
   - Run periodic tests to ensure GPU acceleration remains active
   - Watch for any degradation or CPU fallback

2. **Update Documentation**
   - Document actual GPU performance gains
   - Update projectplan.md with final system status

3. **Production Deployment**
   - System is ready for production use
   - All endpoints functional with GPU acceleration
   - Auto-update system working with Windows Ollama

---

## Quick Reference Commands

```bash
# Test Ollama connectivity
curl http://localhost:11434/api/tags

# Test Enhanced LLM
curl -X POST http://localhost:8080/chat \
  -H "Authorization: Bearer YOUR_API_KEY_HERE" \
  -H "Content-Type: application/json" \
  -d '{"message": "Test", "model": "qwen3:latest"}'

# Run comprehensive tests
cd /mnt/c/BOT/localLLM
python test-windows-ollama-gpu.py

# Check Docker container health
docker ps --format "table {{.Names}}\t{{.Status}}"

# View Enhanced LLM logs
docker logs localllm-enhanced --tail 50

# Restart all services
cd /mnt/c/BOT/localLLM
docker-compose -f docker-compose-simplified.yml restart
```

---

## Files Created/Modified

1. **docker-compose-simplified.yml** - All services point to Windows Ollama
2. **test-windows-ollama-gpu.py** - Comprehensive GPU test suite
3. **WINDOWS_OLLAMA_SETUP_GUIDE.md** - This file
4. **docs/DOCKER_OLLAMA_REMOVAL_STATUS.md** - Detailed removal status

---

**Status:** Ready for Windows Ollama installation completion and testing
**Next Action:** Complete Ollama installation, pull models, run test suite
