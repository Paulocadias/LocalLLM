{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LocalLLM LoRA Training - Google Colab\n",
    "\n",
    "This notebook trains all 6 LoRA profiles for your LocalLLM system.\n",
    "\n",
    "**Profiles**:\n",
    "1. Career Advisor\n",
    "2. Marketing Specialist\n",
    "3. Website Builder\n",
    "4. Android Developer\n",
    "5. Backend Developer\n",
    "6. Frontend Developer\n",
    "\n",
    "**Estimated Time**: 2-4 hours on free T4 GPU\n",
    "\n",
    "**Cost**: Free (Google Colab free tier)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Setup Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check GPU availability\n",
    "!nvidia-smi\n",
    "\n",
    "import torch\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install -q transformers==4.36.0\n",
    "!pip install -q peft==0.7.1\n",
    "!pip install -q trl==0.7.10\n",
    "!pip install -q bitsandbytes==0.41.3\n",
    "!pip install -q accelerate==0.25.0\n",
    "!pip install -q datasets==2.16.1\n",
    "\n",
    "print(\"âœ… All packages installed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Mount Google Drive (to save adapters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Create output directory\n",
    "!mkdir -p /content/drive/MyDrive/localllm_adapters\n",
    "print(\"âœ… Google Drive mounted!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Upload Datasets\n",
    "\n",
    "**Option A**: Upload from your computer (click folder icon on left, drag & drop)\n",
    "\n",
    "**Option B**: Download from GitHub (if you pushed them)\n",
    "\n",
    "Upload these files to `/content/datasets/`:\n",
    "- career_advisor_starter.jsonl\n",
    "- marketing_specialist_starter.jsonl\n",
    "- website_builder_starter.jsonl\n",
    "- android_mobile_starter.jsonl\n",
    "- backend_starter.jsonl\n",
    "- frontend_starter.jsonl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create datasets directory\n",
    "!mkdir -p /content/datasets\n",
    "\n",
    "# If you have a GitHub repo, uncomment and modify:\n",
    "# !git clone https://github.com/Paulocadias/LocalLLM.git\n",
    "# !cp LocalLLM/datasets/lora_profiles/*.jsonl /content/datasets/\n",
    "\n",
    "# Check uploaded files\n",
    "!ls -lh /content/datasets/\n",
    "print(\"\\nâœ… Upload your 6 JSONL files to /content/datasets/ before continuing!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Training Script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile train_lora.py\n",
    "import argparse\n",
    "import os\n",
    "import torch\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    TrainingArguments,\n",
    "    BitsAndBytesConfig\n",
    ")\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "from datasets import load_dataset\n",
    "from trl import SFTTrainer\n",
    "import logging\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "PROFILE_PROMPTS = {\n",
    "    \"career-advisor\": \"You are an expert career advisor specializing in tech careers, job transitions, and professional development.\",\n",
    "    \"marketing-specialist\": \"You are an expert marketing strategist specializing in digital marketing, content strategy, and growth.\",\n",
    "    \"website-builder\": \"You are an expert web designer and developer specializing in high-converting websites and landing pages.\",\n",
    "    \"android\": \"You are an expert Android developer specializing in Kotlin and modern Android development practices.\",\n",
    "    \"backend\": \"You are an expert backend developer specializing in API design, database optimization, and server-side architecture.\",\n",
    "    \"frontend\": \"You are an expert frontend developer specializing in React, TypeScript, and modern web development.\"\n",
    "}\n",
    "\n",
    "def load_model_and_tokenizer(model_name):\n",
    "    bnb_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_quant_type=\"nf4\",\n",
    "        bnb_4bit_compute_dtype=torch.float16,\n",
    "        bnb_4bit_use_double_quant=True,\n",
    "    )\n",
    "    \n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    \n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        quantization_config=bnb_config,\n",
    "        device_map=\"auto\",\n",
    "        trust_remote_code=True,\n",
    "        torch_dtype=torch.float16\n",
    "    )\n",
    "    \n",
    "    model = prepare_model_for_kbit_training(model)\n",
    "    return model, tokenizer\n",
    "\n",
    "def create_lora_config():\n",
    "    return LoraConfig(\n",
    "        r=16,\n",
    "        lora_alpha=32,\n",
    "        target_modules=[\"q_proj\", \"v_proj\", \"k_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
    "        lora_dropout=0.05,\n",
    "        bias=\"none\",\n",
    "        task_type=\"CAUSAL_LM\"\n",
    "    )\n",
    "\n",
    "def prepare_dataset(dataset_path, tokenizer, profile):\n",
    "    dataset = load_dataset('json', data_files=dataset_path, split='train')\n",
    "    system_prompt = PROFILE_PROMPTS.get(profile, \"You are a helpful AI assistant.\")\n",
    "    \n",
    "    def format_prompt(example):\n",
    "        prompt = f\"\"\"<|im_start|>system\\n{system_prompt}<|im_end|>\\n<|im_start|>user\\n{example['instruction']}\"\"\"\n",
    "        if example.get('input'):\n",
    "            prompt += f\"\\n\\nContext: {example['input']}\"\n",
    "        prompt += f\"\"\"<|im_end|>\\n<|im_start|>assistant\\n{example['output']}<|im_end|>\"\"\"\n",
    "        return {\"text\": prompt}\n",
    "    \n",
    "    return dataset.map(format_prompt, remove_columns=dataset.column_names)\n",
    "\n",
    "def fine_tune_lora(profile, dataset_path, output_dir, model_name=\"Qwen/Qwen2.5-7B-Instruct\", epochs=2, batch_size=1):\n",
    "    logger.info(f\"Starting LoRA fine-tuning for profile: {profile}\")\n",
    "    logger.info(f\"Dataset: {dataset_path}\")\n",
    "    logger.info(f\"Output: {output_dir}\")\n",
    "    \n",
    "    # Load model\n",
    "    model, tokenizer = load_model_and_tokenizer(model_name)\n",
    "    \n",
    "    # Configure LoRA\n",
    "    lora_config = create_lora_config()\n",
    "    model = get_peft_model(model, lora_config)\n",
    "    model.print_trainable_parameters()\n",
    "    \n",
    "    # Prepare dataset\n",
    "    dataset = prepare_dataset(dataset_path, tokenizer, profile)\n",
    "    logger.info(f\"Dataset size: {len(dataset)} examples\")\n",
    "    \n",
    "    # Training arguments\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=output_dir,\n",
    "        num_train_epochs=epochs,\n",
    "        per_device_train_batch_size=batch_size,\n",
    "        gradient_accumulation_steps=4,\n",
    "        learning_rate=2e-4,\n",
    "        fp16=True,\n",
    "        logging_steps=1,\n",
    "        save_strategy=\"epoch\",\n",
    "        optim=\"paged_adamw_8bit\",\n",
    "        warmup_ratio=0.1,\n",
    "    )\n",
    "    \n",
    "    # Train\n",
    "    trainer = SFTTrainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=dataset,\n",
    "        dataset_text_field=\"text\",\n",
    "        max_seq_length=2048,\n",
    "        tokenizer=tokenizer,\n",
    "    )\n",
    "    \n",
    "    logger.info(\"Starting training...\")\n",
    "    trainer.train()\n",
    "    \n",
    "    # Save adapter\n",
    "    logger.info(f\"Saving adapter to {output_dir}\")\n",
    "    trainer.model.save_pretrained(output_dir)\n",
    "    tokenizer.save_pretrained(output_dir)\n",
    "    \n",
    "    logger.info(f\"âœ… Training completed for {profile}!\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"--profile\", required=True)\n",
    "    parser.add_argument(\"--dataset\", required=True)\n",
    "    parser.add_argument(\"--output\", required=True)\n",
    "    parser.add_argument(\"--model\", default=\"Qwen/Qwen2.5-7B-Instruct\")\n",
    "    parser.add_argument(\"--epochs\", type=int, default=2)\n",
    "    parser.add_argument(\"--batch-size\", type=int, default=1)\n",
    "    args = parser.parse_args()\n",
    "    \n",
    "    fine_tune_lora(\n",
    "        profile=args.profile,\n",
    "        dataset_path=args.dataset,\n",
    "        output_dir=args.output,\n",
    "        model_name=args.model,\n",
    "        epochs=args.epochs,\n",
    "        batch_size=args.batch_size\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Train All Profiles\n",
    "\n",
    "**This will take 2-4 hours total on T4 GPU**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Profile configurations\n",
    "profiles = [\n",
    "    (\"career-advisor\", \"career_advisor_starter.jsonl\"),\n",
    "    (\"marketing-specialist\", \"marketing_specialist_starter.jsonl\"),\n",
    "    (\"website-builder\", \"website_builder_starter.jsonl\"),\n",
    "    (\"android\", \"android_mobile_starter.jsonl\"),\n",
    "    (\"backend\", \"backend_starter.jsonl\"),\n",
    "    (\"frontend\", \"frontend_starter.jsonl\"),\n",
    "]\n",
    "\n",
    "# Train each profile\n",
    "for profile_name, dataset_file in profiles:\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Training: {profile_name}\")\n",
    "    print(f\"{'='*60}\\n\")\n",
    "    \n",
    "    !python train_lora.py \\\n",
    "        --profile {profile_name} \\\n",
    "        --dataset /content/datasets/{dataset_file} \\\n",
    "        --output /content/drive/MyDrive/localllm_adapters/{profile_name} \\\n",
    "        --model Qwen/Qwen2.5-7B-Instruct \\\n",
    "        --epochs 2 \\\n",
    "        --batch-size 1\n",
    "    \n",
    "    print(f\"\\nâœ… {profile_name} training complete!\\n\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ðŸŽ‰ ALL PROFILES TRAINED SUCCESSFULLY!\")\n",
    "print(\"=\"*60)\n",
    "print(\"\\nAdapters saved to: /content/drive/MyDrive/localllm_adapters/\")\n",
    "print(\"\\nDownload them to your local machine and place in:\")\n",
    "print(\"  C:\\\\BOT\\\\localLLM\\\\lora_adapters\\\\\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Download Adapters\n",
    "\n",
    "Your trained adapters are saved in Google Drive at:\n",
    "`/content/drive/MyDrive/localllm_adapters/`\n",
    "\n",
    "Download each folder:\n",
    "1. career-advisor\n",
    "2. marketing-specialist\n",
    "3. website-builder\n",
    "4. android\n",
    "5. backend\n",
    "6. frontend\n",
    "\n",
    "Place them in: `C:\\BOT\\localLLM\\lora_adapters\\` on your PC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Verify Adapters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check all adapters were created\n",
    "!ls -lh /content/drive/MyDrive/localllm_adapters/\n",
    "\n",
    "# Check adapter contents\n",
    "for profile in [\"career-advisor\", \"marketing-specialist\", \"website-builder\", \"android\", \"backend\", \"frontend\"]:\n",
    "    print(f\"\\n{profile}:\")\n",
    "    !ls /content/drive/MyDrive/localllm_adapters/{profile}/ | head -5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps (On Your Local PC)\n",
    "\n",
    "1. **Download adapters** from Google Drive\n",
    "\n",
    "2. **Place in**: `C:\\BOT\\localLLM\\lora_adapters\\`\n",
    "\n",
    "3. **Create Modelfiles**:\n",
    "```bash\n",
    "cd scripts/lora_profiles\n",
    "./create_modelfiles.sh\n",
    "```\n",
    "\n",
    "4. **Register with Ollama**:\n",
    "```bash\n",
    "./register_models.sh\n",
    "```\n",
    "\n",
    "5. **Test**:\n",
    "```bash\n",
    "ollama run qwen-career-advisor \"How do I negotiate salary?\"\n",
    "```\n",
    "\n",
    "Done! Your system now has 100% quality with LoRA specialization! ðŸš€"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
