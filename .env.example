# LocalLLM Environment Configuration Example
# Copy this file to .env and fill in your values

# ======================
# GitHub API Configuration
# ======================
# Get your GitHub Personal Access Token from: https://github.com/settings/tokens
# Permissions needed: public_repo (for public repositories)
GITHUB_TOKEN=your_github_token_here

# ======================
# Ollama Configuration
# ======================
OLLAMA_BASE_URL=http://host.docker.internal:11434

# ======================
# Redis Configuration
# ======================
REDIS_HOST=redis
REDIS_PORT=6379

# ======================
# Vector DB Configuration
# ======================
VECTOR_DB_URL=http://vector-db:8007
CHROMA_DB_PATH=/app/data/chromadb
EMBEDDING_MODEL=all-MiniLM-L6-v2

# ======================
# LocalLLM Service Configuration
# ======================
# API Key for LocalLLM service - REQUIRED for API authentication
# Generate a secure random key or use: openssl rand -base64 32
LOCALLLM_API_KEY=your_secure_api_key_here

# ======================
# Commercial AI API Configuration (Optional - for comparison testing)
# ======================
# Get your API key from: https://console.anthropic.com/
# Only needed if you want to run Commercial AI comparison tests
# COMMERCIAL_AI_API_KEY=sk-ant-api03-YOUR_KEY_HERE

# ======================
# LoRA Training Configuration
# ======================
# Number of training epochs for LoRA fine-tuning
LORA_EPOCHS=3
# Batch size for training
LORA_BATCH_SIZE=4
# Learning rate
LORA_LEARNING_RATE=2e-4
# LoRA rank (lower = fewer parameters)
LORA_RANK=8
# LoRA alpha (scaling factor)
LORA_ALPHA=16

# ======================
# Dataset Collection Configuration
# ======================
# Number of repositories to sample per profile
GITHUB_REPOS_PER_PROFILE=10
# Minimum stars for repository selection
GITHUB_MIN_STARS=500
# Number of Stack Overflow questions per profile
STACKOVERFLOW_QUESTIONS_PER_PROFILE=50

# ======================
# Model Configuration
# ======================
# Base model for LoRA fine-tuning
BASE_MODEL=Qwen/Qwen2.5-Coder-7B-Instruct
# Device for training (cuda or cpu)
TRAINING_DEVICE=cuda
# Enable 4-bit quantization (recommended for 7B+ models)
USE_QUANTIZATION=true
